{"cells":[{"cell_type":"markdown","metadata":{"id":"WEx2aGrYUKdC"},"source":["### **Sessão e leitura dos dados**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h6FynwEVZ0h"},"outputs":[],"source":["# Iniciando uma sessão\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .master('local[*]')\\\n","    .appName(\"analise_nlp\")\\\n","    .config(\n","    \"spark.driver.extraJavaOptions\",\n","    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\",)\\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8n3r1kieYGx_"},"outputs":[],"source":["dados = spark.read.csv(\"datasets/imdb-reviews-pt-br.csv\",\n","                       escape=\"\\\"\",\n","                       header=True,\n","                       inferSchema=True)"]},{"cell_type":"markdown","metadata":{"id":"wIeTUmHQUU1g"},"source":["### **Explorando os dados**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmmY6AJcXwSd"},"outputs":[],"source":["dados.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(dados.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Linhas: {dados.count()} Colunas: {len(dados.columns)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados.limit(10).show()"]},{"cell_type":"markdown","metadata":{},"source":["Negativo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados.filter(dados.id == 190).select(\"text_pt\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["Positivo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados.filter(dados.id == 12427).select(\"text_pt\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["Quantos comentários negativos e positivos?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados.groupBy(\"sentiment\").count().show()"]},{"cell_type":"markdown","metadata":{},"source":["# Word Cloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","amostra = dados.select(\"text_pt\").sample(fraction=0.10, seed=101)\n","tudo = [texto[\"text_pt\"] for texto in amostra.collect()]\n","\n","wordcloud = WordCloud(width=1000,\n","                        height=600,\n","                        collocations=False,\n","                        prefer_horizontal=1).generate(str(tudo))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(20, 8))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# <b>Limpeza</b>: Caracteres especiais"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","string.punctuation"]},{"cell_type":"markdown","metadata":{},"source":["Exemplo da limpeza"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["amostra = spark.createDataFrame([\n","       (\"Oi, JP! Blz?\",),\n","       (\"$$$\\\\ |~ Parabéns ~| \\\\$$$\",),\n","       (\"(#amovc #paz&amor ^.^)\",),\n","       (\"\\\"bora *_* \\\"\",),\n","       (\"=>->'...``` vc foi selecionad@ ´´´...'<=<-\",),\n","       (\"{comprar: arroz; feijão e pepino}  //\",),\n","       (\"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\",),\n","     (\"Milionário & José Rico\",)\n","], [\"textos\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pyspark.sql.functions as f\n","\n","amostra = amostra.withColumn(\"text_regex\", f.regexp_replace(\"textos\", \"\\$\", \"\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados = dados.withColumn(\"texto_regex\", f.regexp_replace(\"text_en\", \"[\\$#,\\\"!%&'()*+-./;;<=>?@^_`{|}~\\\\\\\\]\", \"\"))\n","\n","dados.limit(2).show(truncate = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dados = dados.withColumn(\"texto_limpo\", f.trim(dados.texto_regex))"]},{"cell_type":"markdown","metadata":{},"source":["# <b>Tokenização</b>: Divisão em tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer\n","\n","tokenizer = Tokenizer(inputCol=\"texto_limpo\", outputCol=\"tokens\")\n","tokenizado = tokenizer.transform(dados)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import IntegerType\n","\n","countTokens = f.udf(lambda tokens: len(tokens), IntegerType())\n","\n","tokenizado.select(\"texto_limpo\", \"tokens\").withColumn(\"Freq_tokens\", countTokens(f.col(\"tokens\")))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizado.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = [(1, 'Spark é ótimo e NLP com Spark é fácil'),\n","                (0, 'Spark MLlib não ajuda muito'),\n","                (1, 'O MLlib do Spark ajuda e é fácil')]\n","\n","colNames =['label', 'texto_limpo']\n","df = spark.createDataFrame(data, colNames)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","nltk.download(\"stopwords\")\n","\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import StopWordsRemover\n","\n","stop_A = stopwords.words(\"portuguese\")\n","stop_B = StopWordsRemover.loadDefaultStopWords(\"portuguese\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer\n","\n","tokenizer = Tokenizer(inputCol = \"texto_limpo\", outputCol= \"tokens\")\n","tokenized = tokenizer.transform(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remover = StopWordsRemover(inputCol = \"tokens\", outputCol= \"texto_final\", stopWords = stop_B)\n","\n","df = remover.transform(tokenized)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remover = StopWordsRemover(inputCol = \"tokens\", outputCol= \"texto_final\", stopWords = stop_A)\n","\n","df = remover.transform(tokenized)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remover = StopWordsRemover(inputCol = \"tokens\", outputCol= \"texto_final\")\n","\n","df = remover.transform(tokenized)\n","feature_data = df"]},{"cell_type":"markdown","metadata":{},"source":["# Bag of words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import CountVectorizer\n","\n","cv = CountVectorizer(inputCol = \"texto_final\", outputCol = \"CountVec\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cv.fit(feature_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","CountVectorizer_features = model.transform(feature_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = cv.fit(feature_data)"]},{"cell_type":"markdown","metadata":{},"source":["# Hashing TF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import HashingTF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hashingTF = HashingTF(inputCol=\"texto_final\", outputCol=\"hashingTF\")\n","hashingTF.setNumFeatures(50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["HTFfeaturizedData  = hashingTF.transform(CountVectorizer_features)"]},{"cell_type":"markdown","metadata":{},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'HTFfeaturizedData' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [35], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m IDF\n\u001b[0;32m      3\u001b[0m idf \u001b[39m=\u001b[39m IDF(inputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhashingTF\u001b[39m\u001b[39m\"\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m idfModel \u001b[39m=\u001b[39m idf\u001b[39m.\u001b[39mfit(HTFfeaturizedData)\n\u001b[0;32m      7\u001b[0m TFIDFfeaturizedData \u001b[39m=\u001b[39m idfModel\u001b[39m.\u001b[39mtransform(HTFfeaturizedData)\n","\u001b[1;31mNameError\u001b[0m: name 'HTFfeaturizedData' is not defined"]}],"source":["from pyspark.ml.feature import IDF\n","\n","idf = IDF(inputCol=\"hashingTF\", outputCol=\"features\")\n","\n","idfModel = idf.fit(HTFfeaturizedData)\n","\n","TFIDFfeaturizedData = idfModel.transform(HTFfeaturizedData)"]},{"cell_type":"markdown","metadata":{},"source":["# Pipeline"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'tdidf' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [36], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m hashingTF \u001b[39m=\u001b[39m HashingTF(inputCol\u001b[39m=\u001b[39mstopwords\u001b[39m.\u001b[39mgetOutputCol(), outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHTF\u001b[39m\u001b[39m\"\u001b[39m, numFeatures\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m      9\u001b[0m tfidf \u001b[39m=\u001b[39m IDF(inputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHTF\u001b[39m\u001b[39m\"\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages \u001b[39m=\u001b[39m [tokenizer, stopwords, hashingTF, tdidf])\n","\u001b[1;31mNameError\u001b[0m: name 'tdidf' is not defined"]}],"source":["# Unindo nossas transformações.\n","from pyspark.ml import Pipeline\n","\n","tokenizer = Tokenizer(inputCol=\"texto_limpo\", outputCol=\"tokens\")\n","stopwords = StopWordsRemover(inputCol=\"tokens\", outputCol=\"texto_final\")\n","hashingTF = HashingTF(inputCol=stopwords.getOutputCol(), outputCol=\"HTF\")\n","tfidf = IDF(inputCol=\"HTF\", outputCol=\"features\")\n","stringIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n","\n","pipeline = Pipeline(stages=[tokenizer, stopwords, hashingTF, tfidf, stringIndexer])\n","\n","dados_transformados = pipeline.fit(dados).transform(dados)"]},{"cell_type":"markdown","metadata":{},"source":["# Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.classification import DecisionTreeClassifier\n","\n","tokenizer = Tokenizer(inputCol = \"texto_limpo\", outputCol = \"tokens\")\n","\n","stopwords = StopWordsRemover(inputCol=\"tokens\", outputCol=\"texto_final\")\n","\n","hashingTF = HashingTF(inputCol=stopwords.getOutputCol(), outputCol=\"HTF\", numFeatures=1000)\n","\n","tfidf = IDF(inputCol=\"HTF\", outputCol=\"features\")\n","\n","dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n","\n","pipeline = Pipeline(stages = [tokenizer, stopwords, hashingTF, tdidf, dt])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train, test = dados.ramdonSplit([0.7, 0.3], seed = 101)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dt_model = pipeline.fit(train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = dt_model.transform(test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName='accuracy')\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Acuracia = %s\" % (accuracy))\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Projeto - Aula 1.2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 ('310_data_science')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"9ca579272762861dcd897c9df5d448a0a7e75b243e644023e3814b6bd73e36eb"}}},"nbformat":4,"nbformat_minor":0}
